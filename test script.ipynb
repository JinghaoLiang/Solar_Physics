{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "#from matplotlib import colors as mc\n",
    "\n",
    "#from PIL import Image\n",
    "from astropy.io import fits\n",
    "from sunpy.map import Map\n",
    "from sunpy.visualization.colormaps import cm\n",
    "#from sunpy.coordinates import frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1601dd",
   "metadata": {},
   "source": [
    "parameters: [image cut level, gradientcut level, smooth scale] = [0.5, 0.3, 40]\n",
    "\n",
    "Channels: $\\mathrm{H}\\alpha$, $\\mathrm{Ca}K$, White Light\n",
    "\n",
    "$\\mathrm{H}\\alpha$: matplotlib.colormaps['sdoaia304'] or mc.LinearSegmentedColormap.from_list('Ha_cmap', [(0, 'black'), (1, 'orange')])\n",
    "\n",
    "$\\mathrm{Ca}K$: matplotlib.colormaps['sdoaia335'] or mc.LinearSegmentedColormap.from_list('CaK_cmap', [(0, 'black'), (1, 'blue')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL = {'save': 'Ha', 'date': 'HA', 'path': 'ha', 'wavelength': 656.28, 'CCD size': 6420, 'guess radius': 2460, 'cmap': matplotlib.colormaps['sdoaia304'], 'notation': r'SZAO H$\\alpha$'}# H\\alpha\n",
    "#CHANNEL = {'save': 'CaK', 'date': 'CAK', 'path': 'cak', 'wavelength': 393.73, 'CCD size': 4208, 'guess radius': 1745, 'cmap': matplotlib.colormaps['sdoaia335'], 'notation': r'SZAO CaK'}# CaK\n",
    "#CHANNEL = {'save': 'WL', 'date': 'WL', 'path': 'wl', 'wavelength': 'White Light', 'CCD size': 4208, 'guess radius': 1745, 'cmap': 'grey', 'notation': r'SZAO WL'}# White Light\n",
    "CCD_size = CHANNEL['CCD size']\n",
    "yyyy = 2025# year\n",
    "date = f'{yyyy}0826{CHANNEL['date']}/'#### ONLY change date\n",
    "local_path = '/home/lenovo/solar_data/'\n",
    "save_matrix_name = os.path.join(local_path, 'alignment/Rotation Matrix', f'{yyyy}0816HA rotation matrix.txt')#### saved at local directory and change ~MONTHLY\n",
    "path = f'/run/user/1000/gvfs/smb-share:server=szaocloud.local,share=solar_observation/{yyyy}/{CHANNEL['path']}/'\n",
    "directory = os.path.join(path, date)\n",
    "save_directory = os.path.join(directory, date)\n",
    "if not os.path.isdir(save_directory):\n",
    "    os.mkdir(save_directory)\n",
    "\n",
    "# load data\n",
    "#fits_files = os.listdir(directory)\n",
    "#for file in fits_files:\n",
    "    #if (('Sun' in file) and ('.fits' in file)):\n",
    "        #file_list.append(os.path.join(directory, file))\n",
    "fits_files = glob.glob(os.path.join(directory, '*.fits'))\n",
    "#file_list = [file for file in fits_files if 'Sun' in file]\n",
    "file_enumerate = list(enumerate([file for file in fits_files if 'Sun' in file], start=1))\n",
    "file_num =len(file_enumerate)\n",
    "print(f'{file_num} .fits files today.')\n",
    "\n",
    "hdrloc_obs = 0\n",
    "hdrloc_gong = 1\n",
    "\n",
    "image_cut_level = 0.5# threshold of luminosity check\n",
    "grad_cut_level = 0.3# threshold of gradient check, for normal:0.4, for bright limbs or dark/shadowed image:0.3\n",
    "smooth_scale = CCD_size / 20# smooth image\n",
    "\n",
    "gong_file = os.path.join(local_path, 'alignment/Reference GONG Maps', '20250816012742Lh.fits.fz')\n",
    "gong_map = Map(gong_file)\n",
    "length = 10\n",
    "origin = 'lower'\n",
    "dpi = 100\n",
    "image_extention = 'jpg'# save image extention\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',# General font family\n",
    "    #'font.serif': 'Times New Roman',# Specific serif font\n",
    "    #'mathtext.fontset': 'custom',\n",
    "    #'mathtext.rm': 'Times New Roman',# Roman (serif) font for math\n",
    "    #'mathtext.it': 'Times New Roman:italic',# Italic\n",
    "    'figure.dpi': dpi,\n",
    "    #'legend.fontsize': legend_size,\n",
    "    'grid.alpha': 0.3,\n",
    "    'savefig.transparent': True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11ce9f",
   "metadata": {},
   "source": [
    "Read fits file, construct UTC string and normalize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fits_file(filename, memmap=True):\n",
    "    hdul = fits.open(filename, memmap=memmap)\n",
    "    hdul.info()\n",
    "    return hdul\n",
    "\n",
    "def check_image_size(input_image):\n",
    "    \"\"\"\n",
    "    In .fits file, Ny=image.shape[0] and Nx=image.shape[1]\n",
    "    \"\"\"\n",
    "    Ny, Nx = input_image.shape\n",
    "    return Ny, Nx, np.min([Ny, Nx])\n",
    "\n",
    "def setup_UTC_string(header, keyword='DATE-AVG'):\n",
    "    date, time = header[keyword].split('T')\n",
    "    year, month, day = date.split('-')\n",
    "    hh, mm, ss_original = time.split(':')\n",
    "    ss = ss_original[0:2]# take the first two digits, DON'T use int(float(ss))-->might omit the '0' in the first digit\n",
    "    time = f'{hh}:{mm}:{ss}'\n",
    "    return date, time, (year, month, day), (hh, mm, ss)\n",
    "\n",
    "def percents(data, pmin=1, pmax=99):\n",
    "    \"\"\"\n",
    "    pmin, pmax: percentile cutoffs (clip outliers)\n",
    "    \"\"\"\n",
    "    if (pmin < 0) or (pmax > 100):\n",
    "        raise ValueError(\"Percentile must be [0, 100]%.\")\n",
    "    \n",
    "    minimum, maximum = np.percentile(data, [pmin, pmax])\n",
    "    data_clipped = np.clip(data, minimum, maximum)\n",
    "    return data_clipped, minimum, maximum\n",
    "\n",
    "def normalize(input_image, pmin=1, pmax=99, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Normalize and enhance contrast of images.\n",
    "    gamma: gamma correction\n",
    "    \"\"\"\n",
    "    image_clipped, vmin, vmax = percents(input_image, pmin, pmax)\n",
    "    image_norm = (image_clipped - vmin) / (vmax - vmin)\n",
    "    return image_norm**gamma\n",
    "\n",
    "from skimage.exposure import rescale_intensity\n",
    "def rescale_image(input_image, out_range=(0,255)):\n",
    "    image_norm = rescale_intensity(input_image, in_range='image', out_range=out_range).astype(np.uint8)\n",
    "    return image_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba5a78",
   "metadata": {},
   "source": [
    ">Check memory usage and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80279b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024**2)# in MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "def aggressive_memory_cleanup(hdul=None, quiet=True):\n",
    "    \"\"\"\n",
    "    Force comprehensive memory cleanup after processing each file\n",
    "    \"\"\"\n",
    "    # Close the FITS file explicitly\n",
    "    if (hdul is not None):\n",
    "        try:\n",
    "            hdul.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Delete any references, the exact variable name in the workflow\n",
    "    local_variables = locals()\n",
    "    for var_name in list(local_variables.keys()):\n",
    "        if ('hdu' in var_name) or ('data' in var_name) or ('header' in var_name) or ('image' in var_name):# including 'hdul'\n",
    "            try:\n",
    "                del local_variables[var_name]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    for _ in range(3):# Force garbage collection multiple times\n",
    "        gc.collect()\n",
    "        time.sleep(0.01)# Small pause between collections\n",
    "\n",
    "    memory_after = get_memory_usage()\n",
    "    if not quiet:\n",
    "        print(f'Memory after cleanup: {memory_after:.1f}MB.')\n",
    "    return memory_after\n",
    "\n",
    "import sys\n",
    "def deep_memory_cleanup():\n",
    "    \"\"\"\n",
    "    Extreme memory cleanup measures\n",
    "    \"\"\"\n",
    "    print('High memory detected - forcing deep cleanup!')\n",
    "    # Clear various caches that might hold memory\n",
    "    try:\n",
    "        np.zeros(1)# Force numpy initialization cleanup\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Multiple garbage collection passes\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        gc.collect()# Double collect each pass\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Clear module-level caches\n",
    "    #cleared = 0\n",
    "    for module in list(sys.modules.values()):\n",
    "        if hasattr(module, '__dict__'):\n",
    "            for key in list(module.__dict__.keys()):\n",
    "                if key.startswith('_cache') or key.endswith('_cache'):\n",
    "                    delattr(module, key)\n",
    "                    #cleared += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4e820",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ffbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_subprocesses():\n",
    "    \"\"\"\n",
    "    Use subprocesses to ensure complete memory isolation\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    import json\n",
    "    \n",
    "    input_files = glob.glob('*.fits')\n",
    "    batch_size = 20\n",
    "    \n",
    "    for i in range(0, len(input_files), batch_size):\n",
    "        batch = input_files[i: i+batch_size]\n",
    "        for file in batch:# Process each file in separate subprocess to guarantee memory cleanup\n",
    "            result = subprocess.run([\n",
    "                'python', '-c', f'''\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from your_processing_module import process_single_file\n",
    "process_single_file(\"{file}\")\n",
    "'''\n",
    "            ], capture_output=True, text=True)\n",
    "\n",
    "            if (result.returncode != 0):\n",
    "                print(f'Error processing {file}: {result.stderr}')\n",
    "\n",
    "# If subprocess approach is too heavy, use this lighter version:\n",
    "def process_with_memory_monitoring():\n",
    "    input_files = glob.glob('*.fits')\n",
    "    high_memory_count = 0\n",
    "\n",
    "    for i, input_file in enumerate(input_files):\n",
    "        memory_before = get_memory_usage()\n",
    "\n",
    "        if (memory_before > 14000):# 14GB threshold\n",
    "            high_memory_count += 1\n",
    "            print(f'High memory before processing: {memory_before:.1f}MB.')\n",
    "\n",
    "            if high_memory_count > 3:# Consider saving state and restarting\n",
    "                print('Consistently high memory - restarting may be needed!')\n",
    "                break\n",
    "\n",
    "        # Process with context manager (automatically closes)\n",
    "        with fits.open(input_file, memmap=True) as hdul:\n",
    "            data = hdul[0].data\n",
    "            processed_data = your_processing_function(data)\n",
    "            \n",
    "            # Write output\n",
    "            output_hdu = fits.PrimaryHDU(processed_data)\n",
    "            output_hdu.header = hdul[0].header\n",
    "            output_hdu.writeto(f'processed_{input_file}', overwrite=True)\n",
    "\n",
    "        aggressive_memory_cleanup()# Force cleanup\n",
    "\n",
    "        if (i % 50 == 0) and (get_memory_usage() > 12000):# If memory is still high after 50 files, suggest restart\n",
    "            print('Consider restarting the process to clear accumulated memory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aef7c1",
   "metadata": {},
   "source": [
    ">Detect clouds\n",
    "\n",
    "mean-->cloud, shadowed, instrument\n",
    "\n",
    "gradient-->cloud\n",
    "\n",
    "morphology-->cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ea299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "from skimage.filters import sobel\n",
    "def clip_shadowed_image(input_image, max_threshold=25000, mean_threshold=8000, gradient_threshold=0.004):\n",
    "    \"\"\"\n",
    "    1. lower mean value, < 8000; while mean value < 1000 is possibly thick clouds or CCD problem\n",
    "    2. lower maximum, < 25000\n",
    "    3. gradient magnitude rather blurred, < 0.005\n",
    "    4. morphology: irregular and expand area, higher ellipcity deviate from 0 (sunspots) but not too high (distinguish from filaments and limbs) and large area\n",
    "    \"\"\"\n",
    "    cloud = False# initialize\n",
    "    # check mean and max value\n",
    "    if (((np.mean(input_image) <= mean_threshold) and (np.max(input_image) <= max_threshold)) or (np.max(input_image) <= 1.1 * max_threshold)):\n",
    "        rough_check = True\n",
    "    else:\n",
    "        rough_check = False\n",
    "    # check gradient magnitude (the result of cv2.Sobel is ~4000 times larger than skimage.filters.sobel)\n",
    "    '''\n",
    "    grad_x = cv2.Sobel(input_image, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(input_image, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    grad_mag = sobel(input_image)\n",
    "    shadow_like = ((input_image > np.max(input_image) * 0.08) & (input_image < mean_threshold * 2)) & (grad_mag < gradient_threshold)# Mask low-gradient dark regions\n",
    "\n",
    "    # check morphology\n",
    "    label_img = label(shadow_like)\n",
    "    morphology_check_list = []\n",
    "    for region in regionprops(label_img):\n",
    "        if ((region.eccentricity < 0.8) and (region.area > 5000)):\n",
    "            # Likely cloud\n",
    "            morphology_check_list.append(True)\n",
    "    cloud = np.any(morphology_check_list) and rough_check\n",
    "    '''\n",
    "    #print(cloud)\n",
    "    return rough_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48677312",
   "metadata": {},
   "source": [
    ">Detect disk center and radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e120972",
   "metadata": {},
   "source": [
    "Hough transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e71653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_hough_circle(input_image, min_center_Dist=3000, minRadius=1800, maxRadius=3000):\n",
    "    \"\"\"\n",
    "    dp: The inverse of the accumulator resolution, use dp=1.0-1.2 for good accuracy.\n",
    "    param1: Used internally by Canny edge detector (acts like threshold1/2), usually =100 is fine.\n",
    "    param2: Circle detection threshold (30-50 for solar data). Higher-->more confident detections, but might miss faint edges; Lower-->more detections, but more false positives.\n",
    "    \"\"\"\n",
    "    image_norm = rescale_image(input_image)\n",
    "    #imgage_equalize = cv2.equalizeHist(image_norm)# Optional: enhance edge contrast\n",
    "    #edges = cv2.Canny(image_norm, threshold1=3, threshold2=6)#after norm, threshold1~6, threshold2~3\n",
    "    edges = cv2.medianBlur(image_norm, 5)\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, dp=1, minDist=min_center_Dist,\n",
    "                               param1=40, param2=50, minRadius=minRadius, maxRadius=maxRadius)\n",
    "    \n",
    "    if (circles is (not None)):\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        xcenter, ycenter, radius = circles[0][0]\n",
    "        return (xcenter, ycenter), radius\n",
    "    else:\n",
    "        raise ValueError(\"No circle found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import canny\n",
    "from skimage.transform import hough_circle, hough_circle_peaks\n",
    "def skimage_hough_circle(input_image, r_range=None):\n",
    "    edges = canny(input_image, sigma=3)\n",
    "\n",
    "    hough_res = hough_circle(edges, r_range)\n",
    "    accums, cx, cy, radii = hough_circle_peaks(hough_res, r_range, total_num_peaks=1)\n",
    "\n",
    "    if (len(cx)==0):\n",
    "        raise ValueError(\"No circle found\")\n",
    "\n",
    "    xcenter, ycenter, rsun = cx[0], cy[0], radii[0]\n",
    "    return (xcenter, ycenter), rsun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fd7ef",
   "metadata": {},
   "source": [
    "Least Square\n",
    "\n",
    "detect and strengthen limbs first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e482c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter, uniform_filter\n",
    "from skimage.filters import sobel\n",
    "def find_limbs(input_image, image_cut_level=0.5, grad_cut_level=0.3, smooth_scale=smooth_scale, initial_radius_guess=CHANNEL['guess radius']):\n",
    "    \"\"\"\n",
    "    1. 6240, image cutlevel=0.5, grad cutlevel=0.3; 2048, image cutlevel=0.6, grad cutlevel=0.4\n",
    "    2. initial guess radius: Halpha (6240) is about 2460; CaK and White Light (4208) are about 1745\n",
    "    \"\"\"\n",
    "    # prepare coordinate grid\n",
    "    Ny, Nx, image_size = check_image_size(input_image)# (Ny, Nx)\n",
    "    x = np.tile(np.arange(Nx), (Ny, 1))\n",
    "    y = np.tile(np.arange(Ny).reshape(-1, 1), (1, Nx))\n",
    "    # margin mask: remove edges\n",
    "    margin = 50\n",
    "    mask = (x >= margin) & (x <= Nx - margin) & (y >= margin) & (y <= Ny - margin)\n",
    "    #input_image *= mask\n",
    "\n",
    "    median_filtered = median_filter(input_image, size=3)# smooth image with median filter\n",
    "    gradient = sobel(median_filtered)# edge detection\n",
    "    gradient_clipped, _, _ = percents(gradient, pmin=1, pmax=99.5)# clip outliers (too strong limbs or spots), within 99.5%\n",
    "    gradient_clipped *= mask\n",
    "    gradient_threshold = grad_cut_level * np.max(gradient_clipped)\n",
    "    std_sob = np.std(gradient_clipped[gradient_clipped > gradient_threshold])# compute robust threshold using std-dev of strong edges\n",
    "\n",
    "    strong_edge = np.copy(gradient_clipped)\n",
    "    strong_edge[gradient_clipped <= gradient_threshold] = -1# find strong edges, if not strong enough, mask by minus value\n",
    "\n",
    "    # smooth original image to suppress center\n",
    "    image_smooth = uniform_filter(input_image.astype(np.float32), size=smooth_scale)\n",
    "    image_smooth *= mask\n",
    "    image_threshold = image_cut_level * np.max(image_smooth)\n",
    "\n",
    "    sa = (strong_edge > 3 * std_sob) & (input_image < image_threshold)#### final mask for candidate limb points\n",
    "    y_raw, x_raw = np.where(sa)\n",
    "    guess_limb = ((x_raw - image_size / 2)**2 + (y_raw - image_size / 2)**2 >= (initial_radius_guess * 0.8)**2) & ((x_raw - image_size / 2)**2 + (y_raw - image_size / 2)**2 <= (initial_radius_guess * 1.2)**2)\n",
    "    y_limb, x_limb = y_raw[guess_limb], x_raw[guess_limb]\n",
    "    limb_points = np.column_stack([y_limb, x_limb])\n",
    "\n",
    "    if (limb_points.shape[0] < 10):\n",
    "        #print(f'number of limb points: {limb_points.shape[0]}')\n",
    "        raise ValueError(\"Too few limb points.\")\n",
    "\n",
    "    return limb_points, y_limb, x_limb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989b322",
   "metadata": {},
   "source": [
    "`skimage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import CircleModel\n",
    "def skimage_leastsq_circle(input_image, image_cut_level=0.6, grad_cut_level=0.4, smooth_scale=40):\n",
    "    \"\"\"\n",
    "    Detect the solar limb using Sobel+median filtering and fit a circle.\n",
    "    cut level: points which value < threshold are out of disk.\n",
    "    \"\"\"\n",
    "    limb_points, _, _ = find_limbs(input_image, image_cut_level=image_cut_level, grad_cut_level=grad_cut_level, smooth_scale=smooth_scale, initial_radius_guess=CHANNEL['guess radius'])\n",
    "\n",
    "    model = CircleModel()\n",
    "    model.estimate(limb_points)\n",
    "    ycenter, xcenter, radius = model.params# note: (row, col, radius)\n",
    "    return (xcenter, ycenter), radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0aad94",
   "metadata": {},
   "source": [
    "`numpy` rough + `scipy` refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48748876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares\n",
    "def fit_circle_algebraic(x, y):\n",
    "    \"\"\"\n",
    "    Fit circle using algebraic least squares (linear) by numpy.\n",
    "    Returns initial guess (x_center, y_center, radius).\n",
    "    \"\"\"\n",
    "    A = np.column_stack((2*x, 2*y, np.ones_like(x)))\n",
    "    b = x**2 + y**2\n",
    "    p, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n",
    "    xc, yc, c = p\n",
    "    r = np.sqrt(xc**2 + yc**2 + c)\n",
    "    return xc, yc, r\n",
    "\n",
    "def scipy_leastsq_circle(input_image, image_cut_level=0.5, grad_cut_level=0.3, smooth_scale=smooth_scale, initial_radius_guess=CHANNEL['guess radius']):\n",
    "    \"\"\"\n",
    "    Nonlinear least-squares fit to a circle.\n",
    "    Parameters:\n",
    "        x, y: coordinates of limb points\n",
    "    Returns:\n",
    "        xc, yc: center coordinates\n",
    "        r: radius\n",
    "    \"\"\"\n",
    "\n",
    "    def residuals(params, x, y):\n",
    "        xcenter, ycenter, r = params\n",
    "        return np.sqrt((x - xcenter)**2 + (y - ycenter)**2) - r\n",
    "\n",
    "    _, y_limb, x_limb = find_limbs(input_image, image_cut_level=image_cut_level, grad_cut_level=grad_cut_level, smooth_scale=smooth_scale, initial_radius_guess=initial_radius_guess)\n",
    "    initial_guess = fit_circle_algebraic(x_limb, y_limb)# initial: use numpy algebraic least squares (linear) fitting\n",
    "    print(f\"Algebraic guess: xc={initial_guess[0]:.4f}, yc={initial_guess[1]:.4f}, r={initial_guess[2]:.4f}\")\n",
    "\n",
    "    refined_fit = least_squares(residuals, initial_guess, args=(x_limb, y_limb))\n",
    "    xcenter, ycenter, radius = refined_fit.x\n",
    "    print(f\"Refined fit: xc={xcenter:.4f}, yc={ycenter:.4f}, r={radius:.4f}\")\n",
    "    return (xcenter, ycenter), radius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d646fce4",
   "metadata": {},
   "source": [
    "linear regression\n",
    "\n",
    "`directly written`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import convolve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def IDL_version_disk_center(input_image, cut_level=0.85, no_log=False):\n",
    "    \"\"\"\n",
    "    Estimate the center and radius of the solar disk from an image. (least sq)\n",
    "\n",
    "    Parameters:\n",
    "        image (2D ndarray): Input image containing the solar limb.\n",
    "        cut_level (float): Threshold ratio for gradient magnitude. Default is 0.45.\n",
    "        no_log (bool): If True, do not apply logarithm to the image.\n",
    "\n",
    "    Returns: (float)\n",
    "        xcenter: X-coordinate of the disk center.\n",
    "        ycenter: Y-coordinate of the disk center.\n",
    "        rsun: Radius of the solar disk.\n",
    "        delxc: Standard deviation in xcenter estimate.\n",
    "        delyc: Standard deviation in ycenter estimate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare coordinate grid\n",
    "    input_shape = input_image.shape# (Ny, Nx)\n",
    "    x = np.tile(np.arange(input_shape[1]), (input_shape[0], 1))\n",
    "    y = np.tile(np.arange(input_shape[0]).reshape(-1, 1), (1, input_shape[1]))\n",
    "\n",
    "    # Sobel kernels\n",
    "    #kernel_x = 0.5 * np.array([[0, 0, 0], [-1, 0, 1], [0, 0, 0]])\n",
    "    #kernel_y = 0.5 * np.array([[0, -1, 0], [0, 0, 0], [0, 1, 0]])\n",
    "    kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "    kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "\n",
    "    # Preprocessing\n",
    "    if no_log:\n",
    "        tmp = input_image\n",
    "    else:\n",
    "        input_image = np.maximum(input_image, np.max(input_image) * 0.1)\n",
    "        tmp = np.log(input_image)\n",
    "\n",
    "    # Compute gradients\n",
    "    gx = convolve(tmp, kernel_x)\n",
    "    gy = convolve(tmp, kernel_y)\n",
    "    g = np.sqrt(gx**2 + gy**2)\n",
    "\n",
    "    # Apply margin mask (remove edges)\n",
    "    margin = 30\n",
    "    mask = (x >= margin) & (x <= input_shape[1] - margin) & (y >= margin) & (y <= input_shape[0] - margin)\n",
    "    g *= mask\n",
    "\n",
    "    # Thresholding for limb detection\n",
    "    threshold = cut_level * np.max(g)\n",
    "    limb_mask = np.where(g >= threshold)\n",
    "\n",
    "    if (limb_mask[0].size <= 10):\n",
    "        raise ValueError(\"Not enough limb points!\")\n",
    "\n",
    "    # Extract gradient components and coordinates at limb points\n",
    "    limb_gx = gx[limb_mask]\n",
    "    limb_gy = gy[limb_mask]\n",
    "    limb_g = g[limb_mask]\n",
    "\n",
    "    qx = -limb_gx / limb_g\n",
    "    qy = -limb_gy / limb_g\n",
    "\n",
    "    x_limb = x[limb_mask]\n",
    "    y_limb = y[limb_mask]\n",
    "    ydata = x_limb * qx + y_limb * qy\n",
    "    xdata = np.vstack((qx, qy)).T\n",
    "\n",
    "    # Linear regression to fit center\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(xdata, ydata)\n",
    "    xcenter, ycenter = reg.coef_\n",
    "    radius = reg.intercept_\n",
    "\n",
    "    # Estimate standard deviation (error)\n",
    "    yfit = reg.predict(xdata)\n",
    "    residuals = ydata - yfit\n",
    "    sigma = np.std(residuals) / np.sqrt(len(residuals))\n",
    "    del_xcenter, del_ycenter = sigma, sigma\n",
    "\n",
    "    return (xcenter, ycenter), np.abs(radius), del_xcenter, del_ycenter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583a618",
   "metadata": {},
   "source": [
    ">Shift disk\n",
    "\n",
    "$\\mathrm{H}\\alpha$ and White Light should `transpose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a96a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_disk(input_image, xcenter, ycenter, channel=CHANNEL['save']):\n",
    "    Ny, Nx, image_size = check_image_size(input_image)# (Ny, Nx) or (height, width)\n",
    "    #print(Ny, Nx)\n",
    "    # (dx, dy): from disk center to image center, x for width and y for height\n",
    "    delta_y = (Ny / 2) - ycenter\n",
    "    delta_x = (Nx / 2) - xcenter\n",
    "    shift_matrix = np.array([[1, 0, delta_x], \n",
    "                             [0, 1, delta_y]], dtype=np.float32)\n",
    "    shift_image = cv2.warpAffine(input_image, shift_matrix, (Nx,Ny))# shift\n",
    "    if (channel=='CaK'):\n",
    "        output_image = np.copy(shift_image)\n",
    "    else:# H\\alpha and White Light\n",
    "        output_image = cv2.resize(shift_image.T, dsize=(image_size,image_size))# transpose\n",
    "    return output_image, shift_matrix, image_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4487a1f",
   "metadata": {},
   "source": [
    ">Affine transform\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{M} = \n",
    "\\begin{bmatrix}\n",
    "    a & b & t_{x}\\\\\n",
    "    c & d & t_{y}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "$a$, $d$: scaling + rotation; $b$, $c$: shearing + rotation; $t_{x}$, $t_{y}$: translation\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "    x'\\\\\n",
    "    y'\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    a & b\\\\\n",
    "    c & d\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "    x\\\\\n",
    "    y\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "    t_{x}\\\\\n",
    "    t_{y}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "for a given center, scale $k$ and rotation angle $\\theta$, OpenCV uses $\\alpha = k\\cos{\\theta}$ and $\\beta = k\\sin{\\theta}$, so \n",
    "\\begin{equation*}\n",
    "\\mathbf{M} = \n",
    "\\begin{bmatrix}\n",
    "    \\alpha & \\beta & (1 - \\alpha)x_{\\mathrm{c}} - \\beta y_{\\mathrm{c}}\\\\\n",
    "    -\\beta & \\alpha & \\beta x_{\\mathrm{c}} + (1 - \\alpha)y_{\\mathrm{c}}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "for only translation, \n",
    "\\begin{equation*}\n",
    "\\mathbf{M} = \n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & t_{x}\\\\\n",
    "    0 & 1 & t_{y}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42967e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_affine_transform(test_image, reference_image, max_features=100, min_keypoints=10):\n",
    "    \"\"\"\n",
    "    Estimates affine transform (rotation + translation) from test_image to reference_image.\n",
    "    Returns transformation matrix M (2x3).\n",
    "    \"\"\"\n",
    "    test_norm = rescale_image(test_image)\n",
    "    reference_norm = rescale_image(reference_image)\n",
    "\n",
    "    # ORB feature detection, description and check.\n",
    "    orb = cv2.ORB_create(nfeatures=max_features)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(test_norm, None)\n",
    "    #print(len(keypoints1))\n",
    "    if ((descriptors1 is None) or (len(keypoints1) < min_keypoints)):\n",
    "        raise ValueError(\"Feature detection failed on the test image.\")\n",
    "\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(reference_norm, None)\n",
    "    #print(len(keypoints2))\n",
    "    if ((descriptors2 is None) or (len(keypoints2) < min_keypoints)):\n",
    "        raise ValueError(\"Feature detection failed on the reference image.\")\n",
    "    \n",
    "    # match descriptors using brute force\n",
    "    brute_force = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = brute_force.match(descriptors1, descriptors2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)# Sort matches by distance (match quality)\n",
    "    print(f'Good matches: {len(matches)}')\n",
    "    if (len(matches) < 3):\n",
    "        raise ValueError(\"Not enough good matches found for alignment.\")\n",
    "\n",
    "    # use good matches to estimate affine transform\n",
    "    src_points = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_points = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    Matrix_estimate, mask = cv2.estimateAffinePartial2D(src_points, dst_points)\n",
    "    #print(Matrix_estimate)\n",
    "    angle_rad = -1 * np.arctan2(Matrix_estimate[1, 0], Matrix_estimate[0, 0])# only extract rotation angle\n",
    "    angle_deg = np.rad2deg(angle_rad)\n",
    "\n",
    "    # create rotation matrix (center given and fixed)\n",
    "    height, width = test_image.shape\n",
    "    center_fixed = (width // 2, height // 2)\n",
    "    Matrix_rotation_only = cv2.getRotationMatrix2D(center_fixed, angle=angle_deg, scale=1.0)\n",
    "    return Matrix_estimate, Matrix_rotation_only, angle_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0903d",
   "metadata": {},
   "source": [
    ">Align image and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9433239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "def align_to_reference(max_features, input_image, reference_image, calculate_matrix=True, transform_matrix=None):\n",
    "    height_i, width_i = input_image.shape\n",
    "    height_r, width_r = reference_image.shape\n",
    "    if calculate_matrix:# if matrix unknown\n",
    "        transform_matrix_rotation_and_scale, transform_matrix_rotation, rotation_angle_degree = estimate_affine_transform(input_image, reference_image, max_features=max_features)\n",
    "        #print(transform_matrix_rotation_and_scale)\n",
    "        #print(transform_matrix_rotation)\n",
    "        aligned_image_output = cv2.warpAffine(input_image, transform_matrix_rotation, (width_i, height_i), flags=cv2.INTER_LINEAR)# transform for output\n",
    "        aligned_image_evaluate = cv2.warpAffine(input_image, transform_matrix_rotation_and_scale, (width_r, height_r), flags=cv2.INTER_LINEAR)# transform for evaluate\n",
    "        score = structural_similarity(rescale_image(aligned_image_evaluate), rescale_image(reference_image))# ssim, evaluate similarity\n",
    "        print(f'Alignment SSIM Score: {score:.4f}')\n",
    "    else:# if already obtained matrix\n",
    "        transform_matrix_rotation = np.copy(transform_matrix)\n",
    "        aligned_image_output = cv2.warpAffine(input_image, transform_matrix_rotation, (width_i, height_i), flags=cv2.INTER_LINEAR)# transform for output\n",
    "        #aligned_image_evaluate = cv2.resize(aligned_image_output, dsize=reference_image.shape)\n",
    "        rotation_angle_degree = None\n",
    "        score = 'Already scored.'\n",
    "    return aligned_image_output, transform_matrix_rotation, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decad2a",
   "metadata": {},
   "source": [
    ">Rewrite header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_header(input_header, input_image, radius, time_obs, date_obs, wavelength=CHANNEL['wavelength'], \n",
    "                  observatory_site='Shenzhen Astronomical Observatory', type_obs='FULLDISK'):\n",
    "    \"\"\"\n",
    "    Changes are automatically saved when using 'update' mode\n",
    "    \"\"\"\n",
    "    output_header = input_header.copy()\n",
    "    naxis2, naxis1, _ = check_image_size(input_image)# (Ny, Nx)\n",
    "    # Only add comment:\n",
    "    output_header.set('Bzero', value=None, comment='Data is Unsigned Integer')# Bzero: Data is Unsigned Integer\n",
    "\n",
    "    # Only add/update keyword:\n",
    "    output_header['CAMERA'] = '          '# CAMERA\n",
    "    output_header['ORIGIN'] = observatory_site# ORIGIN\n",
    "    output_header['TELESCOP'] = '   '# TELESCOP\n",
    "\n",
    "    # Add keyword with comments:\n",
    "    output_header.set('CRPIX1', value=naxis1/2, comment='ThePixelPositionOfTheSolarCenterInTheXAxis')# CRPIX1: ThePixelPositionOfTheSolarCenterInTheXAxis\n",
    "    output_header.set('CRPIX2', value=naxis2/2, comment='ThePixelPositionOfTheSolarCenterInTheYAxis')# CRPIX2: ThePixelPositionOfTheSolarCenterInTheYAxis\n",
    "    output_header.set('DATE_OBS', value=date_obs, comment='observationDate')# DATE_OBS: observationDate\n",
    "    output_header.set('NAXIS1', value=naxis1, comment='length of data axis 1')# NAXIS1: length of data axis 1\n",
    "    output_header.set('NAXIS2', value=naxis2, comment='length of data axis 2')# NAXIS2: length of data axis 2\n",
    "    output_header.set('O_BEZRO', value=np.abs(np.int16(np.copy(input_header['BZERO']))), comment='Original BZERO Value')# O_BEZRO: Original BZERO Value\n",
    "    output_header.set('RSUN_OBS', value=radius, comment='Pixel radius of Sun')# RSUN_OBS: Pixel radius of Sun\n",
    "    output_header.set('TIME-OBS', value=time_obs, comment='observationTime')# TIME-OBS: observationTime\n",
    "    output_header.set('TYPE-OBS', value=type_obs, comment='observationModel')# TYPE-OBS: observationModel\n",
    "    output_header.set('WAVELNTHnm', value=wavelength, comment='waveLength [nm]')# WAVELNTHnm: waveLength [nm]# H\\alpha 656.28; CaK 393.73; WL White Light\n",
    "\n",
    "    # Delete keyword:\n",
    "    if ('ADCBITS' in output_header):# ADCBITS\n",
    "        del output_header['ADCBITS']\n",
    "    if ('BIASADU' in output_header):# BIASADU\n",
    "        del output_header['BIASADU']\n",
    "    if ('BLKLEVEL' in output_header):# BLKLEVEL\n",
    "        del output_header['BLKLEVEL']\n",
    "    if ('CAMID' in output_header):# CAMID\n",
    "        del output_header['CAMID']\n",
    "    if ('DATE-AVG' in output_header):# DATE-AVG\n",
    "        del output_header['DATE-AVG']\n",
    "    if ('EGAIN' in output_header):# EGAIN\n",
    "        del output_header['EGAIN']\n",
    "    if ('EGAINSAV' in output_header):# EGAINSAV\n",
    "        del output_header['EGAINSAV']\n",
    "    if ('GAIN' in output_header):# GAIN\n",
    "        del output_header['GAIN']\n",
    "    if ('JD_UTC' in output_header):# JD_UTC\n",
    "        del output_header['JD_UTC']\n",
    "    if ('OFFSET' in output_header):# OFFSET\n",
    "        del output_header['OFFSET']\n",
    "    \n",
    "    return output_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77439ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coordinate(input_header, reference_header):\n",
    "    delta = np.abs(reference_header['CDELT1'])# angular change (saved in arcsec/px)\n",
    "    R_standard_arcsec = reference_header['SOLAR-R']# arcsec\n",
    "    R_standard_px = R_standard_arcsec / delta# px-->radius in standard image\n",
    "\n",
    "    output_header = input_header.copy()\n",
    "    R_obs_px = input_header['RSUN_OBS']# px\n",
    "    scale = R_obs_px / R_standard_px# from standard GONG to SZAO\n",
    "\n",
    "    output_header.set('WCSNAME', value='Helioprojective-cartesian', comment='WCS system name')\n",
    "    output_header.set('CTYPE1', value='HPLN-TAN', comment='Axis 1 gives helioprojective westward angle')\n",
    "    output_header.set('CTYPE2', value='HPLT-TAN', comment='Axis 2 gives helioprojective northward angle')\n",
    "    output_header.set('CRVAL1', value=0.00000, comment='Helioprojective westward angle at CRPIX1')\n",
    "    output_header.set('CRVAL2', value=0.00000, comment='Helioprojective northward angle at CRPIX2')\n",
    "    output_header.set('CDELT1', value=delta*scale, comment='Angular change per pixel along axis 1')\n",
    "    output_header.set('CDELT2', value=delta*scale, comment='Angular change per pixel along axis 2')\n",
    "    output_header.set('CUNIT1', value='arcsec', comment='Units of CRVAL1 and CDELT1')\n",
    "    output_header.set('CUNIT2', value='arcsec', comment='Units of CRVAL2 and CDELT2')\n",
    "\n",
    "    return output_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf037e1d",
   "metadata": {},
   "source": [
    ">Sharpen edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen_edges(input_image, center, radius, cut_threshold=0.02, factor=2.5, inner_dr=4, outer_dr=0.05*CCD_size, channel=CHANNEL['save']):\n",
    "    \"\"\"\n",
    "    ONLY sharpen H\\alpha images.\n",
    "    6420 image: [factor=2.5, radius range=300]\n",
    "    \"\"\"\n",
    "    if (channel=='Ha'):\n",
    "        if (factor <= 1):\n",
    "            raise ValueError(\"Sharpen factor should be larger than 1.\")\n",
    "\n",
    "        UPPER_LIMIT = np.max(input_image)\n",
    "        (xcenter, ycenter) = center\n",
    "        Ny, Nx, _ = check_image_size(input_image)# (Ny, Nx)\n",
    "        y_grid, x_grid = np.ogrid[:Ny, :Nx]\n",
    "        dist_sq = (x_grid - xcenter)**2 + (y_grid - ycenter)**2\n",
    "\n",
    "        # mask for different radius:\n",
    "        #inside = dist_sq <= (radius - inner_dr)**2\n",
    "        edge_features = (dist_sq > (radius - inner_dr)**2) & (dist_sq <= (radius + outer_dr)**2)\n",
    "        #outside = dist_sq > (radius + outer_dr)**2\n",
    "    \n",
    "        output_image = np.copy(input_image).astype(np.float32)# data type matters for in-place multiplication\n",
    "        #output_image[outside] = 0# remove background\n",
    "        output_image[edge_features & (output_image > cut_threshold * np.max(output_image))] *= factor# sharpen features (keep background regions as the same)\n",
    "        output_image[output_image > UPPER_LIMIT] = UPPER_LIMIT# remove too large values\n",
    "        return output_image\n",
    "    else:# CaK and White Light\n",
    "        return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e1751",
   "metadata": {},
   "source": [
    ">Fit and load rotation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777bdcd",
   "metadata": {},
   "source": [
    "Fit the transform matrix `monthly`\n",
    "\n",
    "rotation matrix is saved as:\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "\\alpha\\\\\n",
    "\\beta\\\\\n",
    "t_{x}\\\\\n",
    "t_{y}\\\\\n",
    "\\theta\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_matrix_file = os.path.join(local_path, 'alignment/Reference GONG Maps', '2025-08-16-0127_1-Sun_00013.fits')\n",
    "observation_file_name = os.path.abspath(fit_matrix_file)\n",
    "hdul = read_fits_file(observation_file_name)\n",
    "data_obs, header_obs = hdul[hdrloc_obs].data, hdul[hdrloc_obs].header# array shape: (ny, nx), while in fits is (nx, ny)\n",
    "_, _, image_size = check_image_size(data_obs)\n",
    "fit_image = data_obs[:image_size, :image_size]# cut a square image (avoid leaving off the last pixel)\n",
    "hdul.close()# free memory\n",
    "\n",
    "(xc_obs, yc_obs), r0_obs = scipy_leastsq_circle(fit_image, image_cut_level=image_cut_level, grad_cut_level=grad_cut_level, smooth_scale=smooth_scale)\n",
    "image_shift, shift_matrix, _ = shift_disk(fit_image, xcenter=xc_obs, ycenter=yc_obs)\n",
    "aligned_image, MATRIX, angle_degree, score = align_to_reference(max_features=100, input_image=image_shift, reference_image=gong_map.data, \n",
    "                                      calculate_matrix=True, transform_matrix=None)\n",
    "\n",
    "alpha, beta = MATRIX[0, 0:2]\n",
    "tx, ty = MATRIX[0:2, 2]\n",
    "components = [alpha, beta, tx, ty, angle_degree]\n",
    "\n",
    "with open(save_matrix_name, 'w') as save_matrix:# Open a file in write mode (\"w\" overwrites, \"a\" appends)\n",
    "    for item in components:\n",
    "        save_matrix.write(f'{item}\\n')\n",
    "save_matrix.close()\n",
    "\n",
    "print(MATRIX)\n",
    "print(f'Rotation matrix saved to: {save_matrix_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70171707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rotation_matrix(file_name, image_size, channel=CHANNEL['save']):\n",
    "    \"\"\"\n",
    "    Rotation matrix is fitted by H\\alpha data originally.\n",
    "    If use it for other channels, further rotation affine should be applied manually.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    with open(file_name, 'r') as load_matrix:\n",
    "        for line in load_matrix:\n",
    "            items.append(np.float64(line))\n",
    "    load_matrix.close()\n",
    "\n",
    "    alpha, beta, tx, ty, rotation_angle_degree = items\n",
    "    #transform_matrix = [[alpha, beta, tx], [-1 * beta, alpha, ty]]\n",
    "\n",
    "    if (channel=='CaK'):\n",
    "        rotation_angle_degree += -69\n",
    "    elif (channel=='WL'):\n",
    "        rotation_angle_degree += 29\n",
    "    transform_matrix = cv2.getRotationMatrix2D((image_size/2, image_size/2), angle=rotation_angle_degree, scale=1.0)\n",
    "    #print(transform_matrix)\n",
    "    return transform_matrix, rotation_angle_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4004c4f",
   "metadata": {},
   "source": [
    "pipeline output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723088a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_transform_matrix, rotation_angle_degree = load_rotation_matrix(file_name=save_matrix_name, image_size=CCD_size)# load rotation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5984f5",
   "metadata": {},
   "source": [
    ">Single file process-->clean up memory after `each` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff79c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_file_process(input_data, input_header, save_extention=image_extention):\n",
    "    fit_image = input_data[:CCD_size, :CCD_size]# crop a SQUARE image (avoid leaving off the last pixel)\n",
    "    date_obs, time_obs, (year, month, day), (hour, minute, second) = setup_UTC_string(input_header, keyword='DATE-AVG')\n",
    "\n",
    "    (xc_obs, yc_obs), r0_obs = scipy_leastsq_circle(fit_image, image_cut_level=image_cut_level, grad_cut_level=grad_cut_level, smooth_scale=smooth_scale)\n",
    "\n",
    "    image_shift, shift_matrix, _ = shift_disk(fit_image, xcenter=xc_obs, ycenter=yc_obs)\n",
    "    aligned_image, _, _ = align_to_reference(max_features=100, input_image=image_shift, reference_image=gong_map.data, \n",
    "                                             calculate_matrix=False, transform_matrix=load_transform_matrix)\n",
    "    #(xc_rot, yc_rot), r0_rot = scipy_leastsq_circle(aligned_image, image_cut_level=image_cut_level, grad_cut_level=grad_cut_level, smooth_scale=smooth_scale)\n",
    "\n",
    "    new_header_obs = update_header(input_header, aligned_image, r0_obs, time_obs, date_obs)\n",
    "    updated_header_obs = add_coordinate(new_header_obs, gong_map.meta)\n",
    "    \n",
    "    file_prefix = f'Z_SWGO_C_BFSE_{year}{month}{day}{hour}{minute}{second}'\n",
    "    output_fits_name = os.path.join(save_directory, f'{file_prefix}_O_{CHANNEL['save']}.fits')\n",
    "    output_jpg_name = os.path.join(save_directory, f'{file_prefix}_P_{CHANNEL['save']}.{save_extention}')# save as .jpg\n",
    "    aligned_image = sharpen_edges(aligned_image, center=(CCD_size/2, CCD_size/2), radius=r0_obs, cut_threshold=0.02, factor=2.5, inner_dr=4)\n",
    "\n",
    "    # plot and save image\n",
    "    pri_hdu = fits.PrimaryHDU(aligned_image.astype(np.uint16), header=updated_header_obs)\n",
    "    pri_hdu.name = ('Primary')\n",
    "    hdu_save = fits.HDUList([pri_hdu])\n",
    "    hdu_save.writeto(output_fits_name, overwrite=True)\n",
    "    ''''''\n",
    "    figure, ax = plt.subplots(1, 1, figsize=(length,length), layout='constrained')\n",
    "    ax.set_position([0, 0, 1, 1])# remove margins\n",
    "    sunplot = ax.imshow(aligned_image, origin=origin, cmap='grey')\n",
    "    #sunplot = ax.imshow(normalize(aligned_image, pmin=1, pmax=99.5, gamma=0.4), origin=origin, cmap=CHANNEL['cmap'])\n",
    "    ax.text(0.01*CCD_size, 0.02*CCD_size, CHANNEL['notation'], color='white', fontsize=40)\n",
    "    ax.text(0.66*CCD_size, 0.95*CCD_size, f'{year}-{month}-{day} {hour}:{minute}:{second} UT', color='white', fontsize=18)\n",
    "    ax.axis('off')\n",
    "    #ax.set_title(utc_string)\n",
    "    #figure.show()\n",
    "    figure.savefig(output_jpg_name)\n",
    "    plt.close(figure)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947fa3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i, file in file_enumerate:\n",
    "    print(f'----######## {i}/{file_num} .fits file ########----')\n",
    "    hdul = read_fits_file(file, memmap=False)# BZERO/BSCALE/BLANK header keywords present. MUST set memmap=False\n",
    "    data_obs, header_obs = hdul[hdrloc_obs].data, hdul[hdrloc_obs].header# array shape: (ny, nx), while in fits is (nx, ny)\n",
    "    if clip_shadowed_image(data_obs):# skip cloud images and go on to the next one\n",
    "        print('----Skip cloud-shadowed data.----')\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        single_file_process(input_data=data_obs, input_header=header_obs)\n",
    "        j += 1\n",
    "        print(f'----######## {j} of {file_num} {CHANNEL['save']} data saved ########----')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {file}: {e}\")\n",
    "    \n",
    "    memory_after = aggressive_memory_cleanup(hdul=hdul)# AGGRESSIVE cleanup after each file\n",
    "    if (i % 50 == 0) and (memory_after > 12000):# Additional check: if memory keeps growing (12GB threshold), take more drastic measures\n",
    "        deep_memory_cleanup()\n",
    "print(f'{j} of {file_num} files are useful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf1594",
   "metadata": {},
   "source": [
    ">Check files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter, uniform_filter\n",
    "from skimage.filters import sobel\n",
    "check_path = os.path.join(local_path, 'check/')\n",
    "testfile = os.path.join(check_path, 'cak/2025-08-25-0131_0-Sun_00001.fits')\n",
    "hdu_test = read_fits_file(testfile)\n",
    "data_test = hdu_test[hdrloc_obs].data[:CCD_size, :CCD_size]\n",
    "hdu_test.close()\n",
    "\n",
    "median_filtered = median_filter(data_test, size=3)\n",
    "g = sobel(median_filtered)# edge detection\n",
    "#print(g.shape, np.max(g), np.mean(g))\n",
    "'''\n",
    "smooth_scale = CCD_size / 20\n",
    "image_smooth = uniform_filter(data_test.astype(np.float32), size=smooth_scale)\n",
    "print(image_smooth.shape, np.max(image_smooth), np.mean(image_smooth))\n",
    "'''\n",
    "#cloud = clip_shadowed_image(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27054146",
   "metadata": {},
   "outputs": [],
   "source": [
    "limb_points, y_limb, x_limb = find_limbs(data_test, image_cut_level=0.5, grad_cut_level=0.3)# normal:0.4, dark/shadowed:0.3, for too strong limbs, fit separately\n",
    "print(x_limb.shape, y_limb.shape)\n",
    "#(xc_obs, yc_obs), r0 = skimage_leastsq_circle(data_test, image_cut_level=image_cut_level)\n",
    "(xc_obs, yc_obs), r0 = scipy_leastsq_circle(data_test, image_cut_level=0.5, grad_cut_level=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shift, shift_matrix, image_size = shift_disk(data_test, xcenter=xc_obs, ycenter=yc_obs)\n",
    "aligned_image, _, score = align_to_reference(input_image=image_shift, reference_image=gong_map.data, calculate_matrix=True, max_features=100)\n",
    "(xc_obs, yc_obs), r0 = scipy_leastsq_circle(aligned_image, image_cut_level=0.5, grad_cut_level=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpen_image = sharpen_edges(aligned_image, center=(CCD_size/2, CCD_size/2), radius=r0_obs, cut_threshold=0.02, factor=2.5, inner_dr=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b16041",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, 2, figsize=(2*length,length), layout='constrained')\n",
    "#sunplot = ax[0].imshow(data_test, origin=origin, cmap='grey')\n",
    "#median = ax[0].imshow(median_filtered, origin=origin, cmap='grey')\n",
    "gradient = ax[0].imshow(g, origin=origin, cmap='grey')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "#shift = ax[1].imshow(sharpen_image, origin=origin, cmap='grey')\n",
    "#smooth = ax[1].imshow(image_smooth, origin=origin, cmap='grey')\n",
    "edge_points = ax[1].scatter(x_limb, y_limb, s=1, marker='x')\n",
    "ax[1].set_xlim(0,CCD_size)\n",
    "ax[1].set_ylim(0,CCD_size)\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "figure.show()\n",
    "'''\n",
    "save_file = os.path.join(directory, 'gradient_check.fits')\n",
    "pri_hdu = fits.PrimaryHDU(g.astype(np.float32))\n",
    "#save_file = os.path.join(directory, 'smooth_check.fits')\n",
    "#pri_hdu = fits.PrimaryHDU(image_smooth)\n",
    "#save_file = os.path.join(directory, 'median.fits')\n",
    "#pri_hdu = fits.PrimaryHDU(median_filtered)\n",
    "pri_hdu.name = ('Primary')\n",
    "hdu_save = fits.HDUList([pri_hdu])\n",
    "hdu_save.writeto(save_file, overwrite=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74317ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.wcs import WCS\n",
    "hdu_gong = read_fits_file(gong_file)\n",
    "data_gong, header_gong = hdu_gong[hdrloc_gong].data, hdu_gong[hdrloc_gong].header\n",
    "hpc = WCS(header_gong)# Helioprojective cartisian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19de461",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import units as u\n",
    "smap = Map(gong_file)\n",
    "fig = plt.figure(figsize=(length,length))\n",
    "ax = fig.add_subplot(projection=smap)\n",
    "smap.plot(axes=ax, clip_interval=(1, 99.99)*u.percent)\n",
    "smap.draw_limb(axes=ax)\n",
    "#smap.draw_grid(axes=ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, 1, figsize=(length,length), subplot_kw={'projection':hpc}, layout='constrained')\n",
    "sunplot = ax.imshow(data_gong, cmap='grey')\n",
    "ax.set_xlabel('Helio X', fontsize=10)\n",
    "ax.set_ylabel('Helio Y', fontsize=10)\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65780ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(frameon=False)\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "ax.set_axis_off()# Disable the axis\n",
    "norm = smap.plot_settings['norm']\n",
    "_, norm.vmin, norm.vmax = percents(smap.data, pmin=1, pmax=99.9)\n",
    "ax.imshow(smap.data, norm=norm, cmap=smap.plot_settings['cmap'], origin=\"lower\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
